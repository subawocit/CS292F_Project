{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5c6a3fed-387c-484b-81ed-f55bb4d8240a",
   "metadata": {},
   "source": [
    "\n",
    "dataset_pretrain for stage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0154a117-039b-4b3d-89b0-ebe720ede08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yuchen/mind-vis/code\n"
     ]
    }
   ],
   "source": [
    "%cd /home/yuchen/mind-vis/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1915e765-1cbd-4732-8dad-da57bca10d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import create_Kamitani_dataset, create_BOLD5000_dataset\n",
    "from os.path import join as pjoin\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn.masking import apply_mask, unmask\n",
    "from nilearn.plotting import plot_epi, plot_stat_map\n",
    "from nilearn.image import load_img, index_img, iter_img\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "from PIL import Image\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b759de-7a1c-4067-9ca2-12b701c7bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = create_Kamitani_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71006d2-a08e-42b8-915c-67f012a47723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 4656)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.fmri.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697d8dbe-0f1e-474e-9697-32f18194f74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load test: 249 out of 250       \n",
      "train_fmri\n",
      "train_img\n",
      "train_img_label_all\n",
      "(3198, 1200)   \n",
      "(3198, 1200)\n",
      "constructing dataset\r"
     ]
    }
   ],
   "source": [
    "_,test = create_Kamitani_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4a72a7-e569-453c-ab77-8ec50ff2b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def pad_to_patch_size(x, patch_size):\n",
    "    assert x.ndim == 2\n",
    "    return np.pad(x, ((0,0),(0, patch_size-x.shape[1]%patch_size)), 'wrap')\n",
    "\n",
    "def normalize(x, mean=None, std=None):\n",
    "    mean = np.mean(x) if mean is None else mean\n",
    "    std = np.std(x) if std is None else std\n",
    "    return (x - mean) / (std * 1.0)\n",
    "\n",
    "class Kamitani_dataset(Dataset):\n",
    "    def __init__(self, fmri, image, img_label, fmri_transform=identity, image_transform=identity, num_voxels=0, num_per_sub=50):\n",
    "        super(Kamitani_dataset, self).__init__()\n",
    "        self.fmri = fmri\n",
    "        self.image = image\n",
    "        if len(self.image) != len(self.fmri):\n",
    "            self.image = np.repeat(self.image, 35, axis=0)\n",
    "        self.fmri_transform = fmri_transform\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = num_voxels\n",
    "        self.num_per_sub = num_per_sub\n",
    "        self.img_class = [i[0] for i in img_label]\n",
    "        self.img_class_name = [i[1] for i in img_label]\n",
    "        self.naive_label = [i[2] for i in img_label]\n",
    "        self.return_image_class_info = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fmri = self.fmri[index]\n",
    "        if index >= len(self.image):\n",
    "            img = np.zeros_like(self.image[0])\n",
    "        else:\n",
    "            img = self.image[index] / 255.0\n",
    "        fmri = np.expand_dims(fmri, axis=0) # (1, num_voxels)\n",
    "        if self.return_image_class_info:\n",
    "            img_class = self.img_class[index]\n",
    "            img_class_name = self.img_class_name[index]\n",
    "            naive_label = torch.tensor(self.naive_label[index])\n",
    "            return {'fmri': self.fmri_transform(fmri), 'image': self.image_transform(img),\n",
    "                    'image_class': img_class, 'image_class_name': img_class_name, 'naive_label':naive_label}\n",
    "        else:\n",
    "            return {'fmri': self.fmri_transform(fmri), 'image': self.image_transform(img)}\n",
    "            \n",
    "def create_Kamitani_dataset(path='../data/Kamitani/npz',  roi='VC', patch_size=16, fmri_transform=identity,\n",
    "            image_transform=identity, subjects = ['sbj_1', 'sbj_2', 'sbj_3', 'sbj_4', 'sbj_5'], \n",
    "            test_category=None, include_nonavg_test=False):\n",
    "    basedir = '/home/yuchen/dataset/fmri'\n",
    "    betas_csv_dir = pjoin(basedir, 'betas_csv')\n",
    "    sub = '01'\n",
    "    \n",
    "    data_file = pjoin(betas_csv_dir, f'sub-{sub}_ResponseData.h5')\n",
    "    responses = pd.read_hdf(data_file)\n",
    "    vox_f = pjoin(betas_csv_dir, f'sub-{sub}_VoxelMetadata.csv')\n",
    "    voxdata = pd.read_csv(vox_f)\n",
    "    stim_f = pjoin(betas_csv_dir, f'sub-{sub}_StimulusMetadata.csv')\n",
    "    stimdata = pd.read_csv(stim_f)\n",
    "\n",
    "    train_idx = stimdata[stimdata['trial_type'] == 'train'].index.tolist()\n",
    "    test_idx = stimdata[stimdata['trial_type'] == 'test'].index.tolist()\n",
    "    \n",
    "    train_fmri = responses[train_idx]\n",
    "    test_fmri = responses[test_idx]\n",
    "    \n",
    "    train_labels = stimdata[stimdata['trial_type'] == 'train']['stimulus'].iloc[:6000]\n",
    "    test_labels = stimdata[stimdata['trial_type'] == 'test']['stimulus'].iloc[:250]\n",
    "    \n",
    "    roi_idx = voxdata[(voxdata['V1'] == 1) | (voxdata['V2'] == 1) | (voxdata['V3'] == 1) | (voxdata['hV4'] == 1) ]['voxel_id'].tolist()\n",
    "    train_fmri = train_fmri.iloc[roi_idx]\n",
    "    test_fmri = test_fmri.iloc[roi_idx]\n",
    "\n",
    "    del responses, voxdata, stimdata\n",
    "    \n",
    "    train_img, test_img = np.array([]),np.array([])\n",
    "\n",
    "    train_img = []  # We will collect arrays here and then concatenate them at the end\n",
    "    first_img_path = os.path.join('/home/yuchen/dataset/images_resized/', train_labels.iloc[0])\n",
    "    with Image.open(first_img_path) as first_img:\n",
    "        first_img_array = np.array(first_img)\n",
    "        img_shape = first_img_array.shape\n",
    "        \n",
    "    train_img = np.empty((len(train_labels), *img_shape), dtype=first_img_array.dtype)\n",
    "    for i, label in enumerate(train_labels):\n",
    "        print(f'load train: {i} out of {len(train_labels)}    ', end='\\r')\n",
    "        img_path = os.path.join('/home/yuchen/dataset/images_resized/', label)\n",
    "        with Image.open(img_path) as img:\n",
    "            train_img[i] = np.array(img)\n",
    "\n",
    "    test_img = []  # We will collect arrays here and then concatenate them at the end\n",
    "    first_img_path = os.path.join('/home/yuchen/dataset/images_resized/', test_labels.iloc[0])\n",
    "    with Image.open(first_img_path) as first_img:\n",
    "        first_img_array = np.array(first_img)\n",
    "        img_shape = first_img_array.shape\n",
    "        \n",
    "    test_img = np.empty((len(test_labels), *img_shape), dtype=first_img_array.dtype)\n",
    "    for i, label in enumerate(test_labels):\n",
    "        print(f'load test: {i} out of {len(test_labels)}    ', end='\\r')\n",
    "        img_path = os.path.join('/home/yuchen/dataset/images_resized/', label)\n",
    "        with Image.open(img_path) as img:\n",
    "            test_img[i] = np.array(img)\n",
    "    print()\n",
    "    train_fmri, test_fmri =  train_fmri.to_numpy(), test_fmri.to_numpy()\n",
    "    train_img_label_all, test_img_label_all = train_labels.tolist(), test_labels.tolist()\n",
    "    num_voxels = train_fmri.shape[-1]\n",
    "\n",
    "    print('normalizing    ', end='\\r')\n",
    "    train_fmri = normalize(pad_to_patch_size(train_fmri, patch_size))\n",
    "    test_fmri = normalize(pad_to_patch_size(test_fmri, patch_size), np.mean(train_fmri), np.std(train_fmri))\n",
    "    \n",
    "    \n",
    "    print('constructing dataset', end = '\\r')\n",
    "    if isinstance(image_transform, list):\n",
    "        return (Kamitani_dataset(train_fmri, train_img, train_img_label_all, fmri_transform, image_transform[0], num_voxels, len(train_img_label_all)//5), \n",
    "                Kamitani_dataset(test_fmri, test_img, test_img_label_all, torch.FloatTensor, image_transform[1], num_voxels, len(test_img_label_all)//5))\n",
    "    else:\n",
    "        return (Kamitani_dataset(train_fmri, train_img, train_img_label_all, fmri_transform, image_transform, num_voxels, len(train_img_label_all)//5), \n",
    "                Kamitani_dataset(test_fmri, test_img, test_img_label_all, torch.FloatTensor, image_transform, num_voxels, len(test_img_label_all)//5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f213d2f-adf0-4b33-8e1e-766571cf235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kamitani_dataset(Dataset):\n",
    "    def __init__(self, fmri, image, img_label, fmri_transform=identity, image_transform=identity, num_voxels=0, num_per_sub=50):\n",
    "        super(Kamitani_dataset, self).__init__()\n",
    "        self.fmri = fmri\n",
    "        self.image = image\n",
    "        if len(self.image) != len(self.fmri):\n",
    "            self.image = np.repeat(self.image, 35, axis=0)\n",
    "        self.fmri_transform = fmri_transform\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = num_voxels\n",
    "        self.num_per_sub = num_per_sub\n",
    "        self.img_class = [i[0] for i in img_label]\n",
    "        self.img_class_name = [i[1] for i in img_label]\n",
    "        self.naive_label = [i[2] for i in img_label]\n",
    "        self.return_image_class_info = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fmri = self.fmri[index]\n",
    "        if index >= len(self.image):\n",
    "            img = np.zeros_like(self.image[0])\n",
    "        else:\n",
    "            img = self.image[index] / 255.0\n",
    "        fmri = np.expand_dims(fmri, axis=0) # (1, num_voxels)\n",
    "        if self.return_image_class_info:\n",
    "            img_class = self.img_class[index]\n",
    "            img_class_name = self.img_class_name[index]\n",
    "            naive_label = torch.tensor(self.naive_label[index])\n",
    "            return {'fmri': self.fmri_transform(fmri), 'image': self.image_transform(img),\n",
    "                    'image_class': img_class, 'image_class_name': img_class_name, 'naive_label':naive_label}\n",
    "        else:\n",
    "            return {'fmri': self.fmri_transform(fmri), 'image': self.image_transform(img)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4579a-118f-4630-8c02-3aeecbae6ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
