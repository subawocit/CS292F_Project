{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5c6a3fed-387c-484b-81ed-f55bb4d8240a",
   "metadata": {},
   "source": [
    "dataset_pretrain for stage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483dc9b3-4e08-4f2e-97d6-90d09d11d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b08592-e6b5-4008-a210-e50626ad9ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.00025, 'min_lr': 0.0, 'weight_decay': 0.05, 'num_epoch': 500, 'warmup_epochs': 40, 'batch_size': 16, 'clip_grad': 0.8, 'mask_ratio': 0.75, 'patch_size': 16, 'embed_dim': 1024, 'decoder_embed_dim': 512, 'depth': 24, 'num_heads': 16, 'decoder_num_heads': 16, 'mlp_ratio': 1.0, 'root_path': '.', 'output_path': './results/fmri_pretrain/07-11-2023-16-41-20', 'seed': 2022, 'roi': 'VC', 'aug_times': 1, 'num_sub_limit': None, 'include_hcp': True, 'include_kam': True, 'accum_iter': 1, 'use_nature_img_loss': False, 'img_recon_weight': 0.5, 'focus_range': None, 'focus_rate': 0.6, 'local_rank': 0}\n",
      "Dataset size: 6360\n",
      "Number of voxels: 4656\n",
      "=======3=======\n",
      "torch.Size([1, 292, 1024])\n",
      "========pos_embed==========\n",
      "torch.Size([1, 292, 1024])\n",
      "(292, 1024)\n",
      "========4==========\n",
      "torch.Size([1, 292, 1024])\n",
      "(292, 1024)\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.00025\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.00025\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Start Training the fmri MAE ... ...\n",
      "========= self.patch_embed(x)before=============\n",
      "torch.Size([16, 1, 4656])\n",
      "=========== self.patch_embed(x) after===========\n",
      "torch.Size([16, 291, 1024])\n",
      "=======pos_embed[:, 1:, :]===============\n",
      "torch.Size([1, 291, 1024])\n",
      "======================\n",
      "torch.Size([16, 291, 1024])\n",
      "===========end===========\n",
      "========= self.patch_embed(x)before=============\n",
      "torch.Size([16, 1, 4656])\n",
      "=========== self.patch_embed(x) after===========\n",
      "torch.Size([16, 291, 1024])\n",
      "=======pos_embed[:, 1:, :]===============\n",
      "torch.Size([1, 291, 1024])\n",
      "======================\n",
      "torch.Size([16, 291, 1024])\n",
      "===========end===========\n",
      "========= self.patch_embed(x)before=============\n",
      "torch.Size([16, 1, 4656])\n",
      "=========== self.patch_embed(x) after===========\n",
      "torch.Size([16, 291, 1024])\n",
      "=======pos_embed[:, 1:, :]===============\n",
      "torch.Size([1, 291, 1024])\n",
      "======================\n",
      "torch.Size([16, 291, 1024])\n",
      "===========end===========\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"code/stageA1_mbm_pretrain.py\", line 269, in <module>\n",
      "    main(config)\n",
      "  File \"code/stageA1_mbm_pretrain.py\", line 192, in main\n",
      "    cor = train_one_epoch(model, dataloader_hcp, optimizer, device, ep, loss_scaler, logger, config, start_time, model_without_ddp,\n",
      "  File \"/home/yuchen/mind-vis/code/sc_mbm/trainer.py\", line 80, in train_one_epoch\n",
      "    loss, pred, _ = model(samples, img_features, valid_idx=valid_idx, mask_ratio=config.mask_ratio)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yuchen/mind-vis/code/sc_mbm/mae_for_fmri.py\", line 322, in forward\n",
      "    pred = self.forward_decoder(latent, ids_restore)  # [N, L, p]\n",
      "  File \"/home/yuchen/mind-vis/code/sc_mbm/mae_for_fmri.py\", line 260, in forward_decoder\n",
      "    x = blk(x)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 229, in forward\n",
      "    x = x + self.drop_path(self.attn(self.norm1(x)))\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 209, in forward\n",
      "    x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python code/stageA1_mbm_pretrain.py --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "384874b0-90b0-4d40-9371-9e3689ef1a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.00025, 'min_lr': 0.0, 'weight_decay': 0.05, 'num_epoch': 500, 'warmup_epochs': 40, 'batch_size': 16, 'clip_grad': 0.8, 'mask_ratio': 0.75, 'patch_size': 16, 'embed_dim': 1024, 'decoder_embed_dim': 512, 'depth': 24, 'num_heads': 16, 'decoder_num_heads': 16, 'mlp_ratio': 1.0, 'root_path': '.', 'output_path': './results/fmri_pretrain/07-11-2023-22-14-40', 'seed': 2022, 'roi': 'VC', 'aug_times': 1, 'num_sub_limit': None, 'include_hcp': True, 'include_kam': True, 'accum_iter': 1, 'use_nature_img_loss': False, 'img_recon_weight': 0.5, 'focus_range': None, 'focus_rate': 0.6, 'local_rank': 0, 'dataset': 'things', 'subject': '01'}\n",
      "Dataset size: 3198\n",
      "Number of voxels: 8656\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.00025\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.00025\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Start Training the fmri MAE ... ...\n",
      "[Epoch 0] loss: 1.2402460065484047\n",
      "Saving best cor model\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"code/stageA1_mbm_pretrain.py\", line 276, in <module>\n",
      "    main(config)\n",
      "  File \"code/stageA1_mbm_pretrain.py\", line 199, in main\n",
      "    cor = train_one_epoch(model, dataloader_hcp, optimizer, device, ep, loss_scaler, logger, config, start_time, model_without_ddp,\n",
      "  File \"/home/yuchen/mind-vis/code/sc_mbm/trainer.py\", line 92, in train_one_epoch\n",
      "    loss_scaler(loss, optimizer, parameters=model.parameters(), clip_grad=config.clip_grad)\n",
      "  File \"/home/yuchen/mind-vis/code/sc_mbm/trainer.py\", line 24, in __call__\n",
      "    self._scaler.step(optimizer)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py\", line 416, in step\n",
      "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py\", line 314, in _maybe_opt_step\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py\", line 314, in <genexpr>\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python code/stageA1_mbm_pretrain.py --batch_size 16 --subject 01 --patch_size 16 --dataset things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0585574f-66ca-4013-a936-7f8f455bea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root_path': '.', 'output_path': './results/fmri_finetune/07-11-2023-22-38-47', 'kam_path': './data/Kamitani/npz', 'bold5000_path': './data/BOLD5000', 'dataset': 'things', 'pretrain_mbm_path': './pretrains/GOD/fmri_encoder.pth', 'include_nonavg_test': True, 'kam_subs': ['sbj_3'], 'bold5000_subs': ['CSI1'], 'things_subs': '01', 'lr': 5.3e-05, 'weight_decay': 0.05, 'num_epoch': 15, 'batch_size': 16, 'mask_ratio': 0.75, 'accum_iter': 1, 'clip_grad': 0.8, 'warmup_epochs': 2, 'min_lr': 0.0, 'local_rank': 0, 'subject': '01', 'roi': 'VC'}\n",
      "(3198, 1216)\n",
      "Dataset size: 3198\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5.3e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5.3e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Finetuning MAE on test fMRI ... ...\n",
      "[Epoch 0] loss: 0.004994421461597085\n",
      "Saving best cor model\n",
      "[Epoch 1] loss: 0.00440866265911609\n",
      "Saving best cor model\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"code/stageA2_mbm_finetune.py\", line 250, in <module>\n",
      "    main(config)\n",
      "  File \"code/stageA2_mbm_finetune.py\", line 176, in main\n",
      "    cor = train_one_epoch(model, dataloader_hcp, optimizer, device, ep, loss_scaler, logger, config, start_time, model_without_ddp)\n",
      "  File \"/home/yuchen/mind-vis/code/sc_mbm/trainer.py\", line 96, in train_one_epoch\n",
      "    pred = pred.to('cpu').detach()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python code/stageA2_mbm_finetune.py --dataset things --batch_size 16 --subject 01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d06c555-9a95-4c6c-8158-df72692d7d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 401.32 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Position interpolate from 262 to 541\n",
      "missing keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n",
      "unexpected keys: ['mask_token']\n",
      "rendering 5 examples in 250 steps.\n",
      "Data shape for PLMS sampling is (5, 3, 64, 64)\n",
      "Running PLMS Sampling with 250 timesteps\n",
      "PLMS Sampler: 100%|███████████████████████████| 250/250 [00:38<00:00,  6.41it/s]\n",
      "rendering 5 examples in 250 steps.\n",
      "Data shape for PLMS sampling is (5, 3, 64, 64)\n",
      "Running PLMS Sampling with 250 timesteps\n",
      "PLMS Sampler:   4%|█▏                          | 11/250 [00:01<00:37,  6.37it/s]^C\n",
      "PLMS Sampler:   4%|█▏                          | 11/250 [00:01<00:40,  5.84it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"code/stageB_ldm_finetune.py\", line 291, in <module>\n",
      "    main(config)\n",
      "  File \"code/stageB_ldm_finetune.py\", line 208, in main\n",
      "    generate_images(generative_model, fmri_latents_dataset_train, fmri_latents_dataset_test, config)\n",
      "  File \"code/stageB_ldm_finetune.py\", line 88, in generate_images\n",
      "    grid, samples = generative_model.generate(fmri_latents_dataset_train, config.num_samples, \n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/ldm_for_fmri.py\", line 149, in generate\n",
      "    samples_ddim, _ = sampler.sample(S=ddim_steps, \n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/models/diffusion/plms.py\", line 97, in sample\n",
      "    samples, intermediates = self.plms_sampling(conditioning, size,\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/models/diffusion/plms.py\", line 153, in plms_sampling\n",
      "    outs = self.p_sample_plms(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/models/diffusion/plms.py\", line 219, in p_sample_plms\n",
      "    e_t = get_model_output(x, t)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/models/diffusion/plms.py\", line 181, in get_model_output\n",
      "    e_t = self.model.apply_model(x, t, c)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/models/diffusion/ddpm.py\", line 1044, in apply_model\n",
      "    x_recon = self.model(x_noisy, t, **cond)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/models/diffusion/ddpm.py\", line 1482, in forward\n",
      "    out = self.diffusion_model(x, t, context=cc)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/modules/diffusionmodules/openaimodel.py\", line 756, in forward\n",
      "    h = module(h, emb, context)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/modules/diffusionmodules/openaimodel.py\", line 85, in forward\n",
      "    x = layer(x, emb)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/modules/diffusionmodules/openaimodel.py\", line 252, in forward\n",
      "    return checkpoint(\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/modules/diffusionmodules/util.py\", line 116, in checkpoint\n",
      "    return func(*inputs)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/modules/diffusionmodules/openaimodel.py\", line 276, in _forward\n",
      "    h = self.out_layers(h)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yuchen/mind-vis/code/dc_ldm/modules/diffusionmodules/util.py\", line 216, in forward\n",
      "    return super().forward(x.float()).type(x.dtype)\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/modules/normalization.py\", line 279, in forward\n",
      "    return F.group_norm(\n",
      "  File \"/home/yuchen/miniconda3/envs/mind-vis/lib/python3.8/site-packages/torch/nn/functional.py\", line 2558, in group_norm\n",
      "    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python code/stageB_ldm_finetune.py --dataset things --subject 01 --roi VC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e78a3e-994b-44fc-996b-ffc083c336ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dbfc1-1592-46eb-8420-e288738f39d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6499fa-0a35-497a-bec3-00513a23a63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153304e2-b357-41f6-88eb-abe618db2715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8476d393-ab2f-47e4-b2a4-f0a6f9c7eb9c",
   "metadata": {},
   "source": [
    "# actual code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0154a117-039b-4b3d-89b0-ebe720ede08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yuchen/mind-vis/code\n"
     ]
    }
   ],
   "source": [
    "%cd /home/yuchen/mind-vis/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1915e765-1cbd-4732-8dad-da57bca10d17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_Kamitani_dataset, create_BOLD5000_dataset, hcp_dataset, create_things_dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m join \u001b[38;5;28;01mas\u001b[39;00m pjoin\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "from dataset import create_Kamitani_dataset, create_BOLD5000_dataset, hcp_dataset, create_things_dataset\n",
    "from os.path import join as pjoin\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn.masking import apply_mask, unmask\n",
    "from nilearn.plotting import plot_epi, plot_stat_map\n",
    "from nilearn.image import load_img, index_img, iter_img\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "from PIL import Image\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4a3e3-f880-4ccb-a61f-f36f1ddb4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = hcp_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15e7f8-355a-4092-9eb6-da3a7736e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]['fmri'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b759de-7a1c-4067-9ca2-12b701c7bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,test = create_Kamitani_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0a69426-de88-4339-815c-123db03fec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_1 = create_things_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0731cd61-b26e-40bc-8669-69a3ba1f2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1,_ = test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8170b73b-aeb9-4b73-a87b-c9ee77c279d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8656)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1[0]['fmri'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d8dbe-0f1e-474e-9697-32f18194f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _ = create_things_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c4a72a7-e569-453c-ab77-8ec50ff2b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def pad_to_patch_size(x, patch_size):\n",
    "    assert x.ndim == 2\n",
    "    return np.pad(x, ((0,0),(0, patch_size-x.shape[1]%patch_size)), 'wrap')\n",
    "\n",
    "def normalize(x, mean=None, std=None):\n",
    "    mean = np.mean(x) if mean is None else mean\n",
    "    std = np.std(x) if std is None else std\n",
    "    return (x - mean) / (std * 1.0)\n",
    "\n",
    "class things_dataset(Dataset):\n",
    "    def __init__(self, fmri, image, img_label, fmri_transform=identity, image_transform=identity, num_voxels=0, num_per_sub=50):\n",
    "        super(things_dataset, self).__init__()\n",
    "        self.fmri = fmri\n",
    "        self.image = image\n",
    "        if len(self.image) != len(self.fmri):\n",
    "            self.image = np.repeat(self.image, 35, axis=0)\n",
    "        self.fmri_transform = fmri_transform\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = num_voxels\n",
    "        self.num_per_sub = num_per_sub\n",
    "        self.img_class = [i[0] for i in img_label]\n",
    "        self.img_class_name = [i[1] for i in img_label]\n",
    "        self.naive_label = [i[2] for i in img_label]\n",
    "        self.return_image_class_info = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fmri = self.fmri[index]\n",
    "        if index >= len(self.image):\n",
    "            img = np.zeros_like(self.image[0])\n",
    "        else:\n",
    "            img = self.image[index] / 255.0\n",
    "        fmri = np.expand_dims(fmri, axis=0) # (1, num_voxels)\n",
    "        if self.return_image_class_info:\n",
    "            img_class = self.img_class[index]\n",
    "            img_class_name = self.img_class_name[index]\n",
    "            naive_label = torch.tensor(self.naive_label[index])\n",
    "            return {'fmri': self.fmri_transform(fmri), 'image': self.image_transform(img),\n",
    "                    'image_class': img_class, 'image_class_name': img_class_name, 'naive_label':naive_label}\n",
    "        else:\n",
    "            return {'fmri': self.fmri_transform(fmri), 'image': self.image_transform(img)}\n",
    "            \n",
    "def create_things_dataset(path='../data/Kamitani/npz',  roi='VC', patch_size=16, fmri_transform=identity,\n",
    "            image_transform=identity, subjects = ['sbj_1', 'sbj_2', 'sbj_3', 'sbj_4', 'sbj_5'], \n",
    "            test_category=None, include_nonavg_test=False):\n",
    "    basedir = '/home/yuchen/dataset/fmri'\n",
    "    betas_csv_dir = pjoin(basedir, 'betas_csv')\n",
    "    sub = '01'\n",
    "    \n",
    "    data_file = pjoin(betas_csv_dir, f'sub-{sub}_ResponseData.h5')\n",
    "    responses = pd.read_hdf(data_file)\n",
    "    vox_f = pjoin(betas_csv_dir, f'sub-{sub}_VoxelMetadata.csv')\n",
    "    voxdata = pd.read_csv(vox_f)\n",
    "    stim_f = pjoin(betas_csv_dir, f'sub-{sub}_StimulusMetadata.csv')\n",
    "    stimdata = pd.read_csv(stim_f)\n",
    "\n",
    "    train_idx = stimdata[stimdata['trial_type'] == 'train'].index.tolist()\n",
    "    test_idx = stimdata[stimdata['trial_type'] == 'test'].index.tolist()\n",
    "    \n",
    "    train_fmri = responses[train_idx]\n",
    "    test_fmri = responses[test_idx]\n",
    "    \n",
    "    train_labels = stimdata[stimdata['trial_type'] == 'train']['stimulus'].iloc[:6000]\n",
    "    test_labels = stimdata[stimdata['trial_type'] == 'test']['stimulus'].iloc[:250]\n",
    "    \n",
    "    roi_idx = voxdata[(voxdata['V1'] == 1) | (voxdata['V2'] == 1) | (voxdata['V3'] == 1) | (voxdata['hV4'] == 1) ]['voxel_id'].tolist()\n",
    "    train_fmri = train_fmri.iloc[roi_idx]\n",
    "    test_fmri = test_fmri.iloc[roi_idx]\n",
    "\n",
    "    del responses, voxdata, stimdata\n",
    "    \n",
    "    train_img, test_img = np.array([]),np.array([])\n",
    "\n",
    "    train_img = []  # We will collect arrays here and then concatenate them at the end\n",
    "    first_img_path = os.path.join('/home/yuchen/dataset/images_resized/', train_labels.iloc[0])\n",
    "    with Image.open(first_img_path) as first_img:\n",
    "        first_img_array = np.array(first_img)\n",
    "        img_shape = first_img_array.shape\n",
    "        \n",
    "    train_img = np.empty((len(train_labels), *img_shape), dtype=first_img_array.dtype)\n",
    "    for i, label in enumerate(train_labels):\n",
    "        img_path = os.path.join('/home/yuchen/dataset/images_resized/', label)\n",
    "        with Image.open(img_path) as img:\n",
    "            train_img[i] = np.array(img)\n",
    "\n",
    "    test_img = []  # We will collect arrays here and then concatenate them at the end\n",
    "    first_img_path = os.path.join('/home/yuchen/dataset/images_resized/', test_labels.iloc[0])\n",
    "    with Image.open(first_img_path) as first_img:\n",
    "        first_img_array = np.array(first_img)\n",
    "        img_shape = first_img_array.shape\n",
    "        \n",
    "    test_img = np.empty((len(test_labels), *img_shape), dtype=first_img_array.dtype)\n",
    "    for i, label in enumerate(test_labels):\n",
    "        img_path = os.path.join('/home/yuchen/dataset/images_resized/', label)\n",
    "        with Image.open(img_path) as img:\n",
    "            test_img[i] = np.array(img)\n",
    "    print()\n",
    "    train_fmri, test_fmri =  train_fmri.to_numpy(), test_fmri.to_numpy()\n",
    "    train_img_label_all, test_img_label_all = train_labels.tolist(), test_labels.tolist()\n",
    "    num_voxels = train_fmri.shape[-1]\n",
    "\n",
    "    train_fmri = normalize(pad_to_patch_size(train_fmri, patch_size))\n",
    "    test_fmri = normalize(pad_to_patch_size(test_fmri, patch_size), np.mean(train_fmri), np.std(train_fmri))\n",
    "    \n",
    "    if isinstance(image_transform, list):\n",
    "        return (things_dataset(train_fmri, train_img, train_img_label_all, fmri_transform, image_transform[0], num_voxels, len(train_img_label_all)//5), \n",
    "                things_dataset(test_fmri, test_img, test_img_label_all, torch.FloatTensor, image_transform[1], num_voxels, len(test_img_label_all)//5))\n",
    "    else:\n",
    "        return (things_dataset(train_fmri, train_img, train_img_label_all, fmri_transform, image_transform, num_voxels, len(train_img_label_all)//5), \n",
    "                things_dataset(test_fmri, test_img, test_img_label_all, torch.FloatTensor, image_transform, num_voxels, len(test_img_label_all)//5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f213d2f-adf0-4b33-8e1e-766571cf235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class things_dataset(Dataset):\n",
    "    def __init__(self, fmri, image, img_label, fmri_transform=identity, image_transform=identity, num_voxels=0, num_per_sub=50):\n",
    "        super(Kamitani_dataset, self).__init__()\n",
    "        self.fmri = fmri\n",
    "        self.image = image\n",
    "        if len(self.image) != len(self.fmri):\n",
    "            self.image = np.repeat(self.image, 35, axis=0)\n",
    "        self.fmri_transform = fmri_transform\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = num_voxels\n",
    "        self.num_per_sub = num_per_sub\n",
    "        self.img_class = [i[0] for i in img_label]\n",
    "        self.img_class_name = [i[1] for i in img_label]\n",
    "        self.naive_label = [i[2] for i in img_label]\n",
    "        self.return_image_class_info = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fmri = self.fmri[index]\n",
    "        if index >= len(self.image):\n",
    "            img = np.zeros_like(self.image[0])\n",
    "        else:\n",
    "            img = self.image[index] / 255.0\n",
    "        fmri = np.expand_dims(fmri, axis=0) # (1, num_voxels)\n",
    "        if self.return_image_class_info:\n",
    "            img_class = self.img_class[index]\n",
    "            img_class_name = self.img_class_name[index]\n",
    "            naive_label = torch.tensor(self.naive_label[index])\n",
    "            return {'fmri': self.fmri_transform(fmri), 'image': self.image_transform(img),\n",
    "                    'image_class': img_class, 'image_class_name': img_class_name, 'naive_label':naive_label}\n",
    "        else:\n",
    "            return {'fmri': self.fmri_transform(fmri), 'image': self.image_transform(img)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4579a-118f-4630-8c02-3aeecbae6ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
